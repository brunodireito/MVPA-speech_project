{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunomiguel/.local/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nilearn\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import datasets\n",
    "from nilearn import image\n",
    "from nilearn.image import mean_img\n",
    "\n",
    "# Let us use a Nifti file that is shipped with nilearn\n",
    "from nilearn.datasets import MNI152_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/brunomiguel/Documents/GitHub/MVPA-speech_project/MVPA-project-admin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "ROOT_DATA='/Users/home/Documents/BIDS/'\n",
    "SUB='sub-0001'\n",
    "SES='ses-001'\n",
    "\n",
    "# data folder\n",
    "data_path=os.path.join(root_data,sub,ses)\n",
    "print('The data is in this folder - ' + data_path)\n",
    "\n",
    "# project folder\n",
    "ROOT_PROJECT=\"/Users/home/Documents/GitHub/MVPA-speech_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural data.\n",
    "\n",
    "## Check registration using FSL BET (brain extraction) + FEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1w_file_path=data_path+'/anat/sub-0001_ses-001_run-01_T1w.nii.gz'\n",
    "plotting.plot_img(t1w_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1w_brain_file_path='/Users/home/Documents/BIDS/sub-0001/ses-001/anat/sub-0001_ses-001_run-01_T1w_brain.nii.gz'\n",
    "\n",
    "plotting.plot_img(t1w_brain_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1w_prepro_file_path='/Users/home/Documents/BIDS/sub-0001/ses-001/run-1.feat/reg/highres2standard.nii.gz'\n",
    "\n",
    "cut_coords=(0, 0, 0)\n",
    "plotting.plot_img(t1w_prepro_file_path, cut_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0,0,0) no standard MNI (mas tb em Talairach) aponta sempre para uma estrutura anat√≥mica chamada de comissura anterior.\n",
    "plotting.plot_img(datasets.MNI152_FILE_PATH, cut_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional data.\n",
    "## Check preprocessing using FSL FEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_data_preproc='/Users/home/Documents/BIDS/sub-0001/ses-001/run-1.feat/filtered_func_data.nii.gz'\n",
    "\n",
    "mean_fmri_img=mean_img(func_data_preproc)\n",
    "plotting.plot_img(mean_fmri_img, cut_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_data_preproc_2high='/Users/home/Documents/BIDS/sub-0001/ses-001/run-1.feat/reg/example_func2standard.nii.gz'\n",
    "\n",
    "mean_fmri_img_2high=mean_img(func_data_preproc_2high)\n",
    "\n",
    "print(image.load_img(func_data_preproc_2high).shape)\n",
    "\n",
    "plotting.plot_img(mean_fmri_img_2high, cut_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_data_preproc2standard='/Users/home/Documents/BIDS/sub-0001/ses-001/run-1.feat/filtered_func_data2standard.nii.gz'\n",
    "print(image.load_img(func_data_preproc2standard).shape)\n",
    "\n",
    "mean_fmri_img_2standard=mean_img(func_data_preproc2standard)\n",
    "plotting.plot_img(mean_fmri_img_2standard, cut_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the events and design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "tr = 2.5  # repetition time is ? second\n",
    "n_scans = 222  # the acquisition comprises ?? scans\n",
    "frame_times = np.arange(n_scans) * tr  # here are the correspoding frame times\n",
    "\n",
    "# load events.tsv\n",
    "events_PATH='/Users/home/Documents/GitHub/MVPA-speech_project/convert-bids-admin/DOCKER/events.tsv'\n",
    "\n",
    "events_df = pd.read_csv(events_PATH, sep='\\t', na_values=\"n/a\")\n",
    "print(events_df.head())\n",
    "\n",
    "print(frame_times[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load events.tsv\n",
    "events_PATH='/Users/home/Documents/GitHub/MVPA-speech_project/convert-bids-admin/DOCKER/betaseries_fonologico.tsv'\n",
    "\n",
    "events_df = pd.read_csv(events_PATH, sep='\\t', na_values=\"n/a\")\n",
    "print(events_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_model='spm'\n",
    "design_matrix = make_first_level_design_matrix(frame_times, events_df,\n",
    "                                    drift_model='polynomial', drift_order=3,\n",
    "                                    hrf_model=hrf_model)\n",
    "\n",
    "plot_design_matrix(design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical analysis - 1st level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_matrix = np.eye(design_matrix.shape[1])\n",
    "basic_contrasts = dict([(column, contrast_matrix[i])\n",
    "                        for i, column in enumerate(design_matrix.columns)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "print('Fitting a GLM')\n",
    "fmri_glm = FirstLevelModel()\n",
    "fmri_glm.fit(func_data_preproc2standard, design_matrices=design_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_maps=[]\n",
    "for contrast in basic_contrasts:\n",
    "    z_maps.append(fmri_glm.compute_contrast(basic_contrasts[contrast], output_type='z_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the contrasts \n",
    "# the display is overlayed on the mean fMRI image\n",
    "# a threshold of 3.0 is used, more sophisticated choices are possible\n",
    "plotting.plot_glass_brain(\n",
    "    z_maps[10], threshold=3.0, title='test')\n",
    "plotting.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(z_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain_mask = datasets.load_mni152_brain_mask()\n",
    "\n",
    "from nilearn.datasets import fetch_icbm152_brain_gm_mask\n",
    "brain_mask = fetch_icbm152_brain_gm_mask()\n",
    "\n",
    "\n",
    "dataset = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm') \n",
    "atlas_filename = dataset.maps\n",
    "\n",
    "frontal_mask = mean_img([math_img('img == %d'  %i, img=atlas_filename) for i in [48]])\n",
    "plotting.view_img(frontal_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import Decoder \n",
    "\n",
    "decoder = Decoder(estimator='svc', mask=frontal_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(frontal_mask, bg_img=mean_fmri_img_2standard,\n",
    "                  cmap='Paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input are z_maps\n",
    "\n",
    "# load events.tsv\n",
    "events_PATH='/Users/home/Documents/GitHub/MVPA-speech_project/convert-bids-admin/DOCKER/events_epoch_fonologico.tsv'\n",
    "\n",
    "events_df = pd.read_csv(events_PATH, sep='\\t', na_values=\"n/a\")\n",
    "print(events_df.head(10))\n",
    "\n",
    "conditions=events_df['trial_type'][:216]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conditions)\n",
    "print(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import index_img\n",
    "trainset = index_img(z_maps, slice(0, -54))\n",
    "trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.fit(trainset, conditions[:-50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder.cv_scores_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = index_img(z_maps, slice(-54,-4))\n",
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=decoder.predict(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((prediction == conditions[-50:]).sum() / float(len(conditions[-50:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(\n",
    "    decoder.coef_img_['Task '], bg_img=t1w_prepro_file_path,\n",
    "    title=\"SVM weights\", dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dummy decoder - como avaliar a qualidade de um classificador?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_decoder = Decoder(estimator='dummy_classifier', mask=brain_mask)\n",
    "\n",
    "dummy_decoder.fit(trainset, conditions[:-50]) \n",
    "\n",
    "dummy_prediction=dummy_decoder.predict(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_decoder.dummy_output_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "atlas_filename = dataset.maps\n",
    "labels = dataset.labels\n",
    "\n",
    "#yeo = datasets.fetch_atlas_yeo_2011()\n",
    "#atlas_filename=yeo['thick_17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yeo.description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = plotting.find_parcellation_cut_coords(atlas_filename)\n",
    "plotting.plot_roi(atlas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import index_img\n",
    "\n",
    "baseline_imgs = index_img(func_data_preproc2standard, baselineidxs)\n",
    "task_imgs = index_img(func_data_preproc2standard, taskidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_series = masker.fit_transform(baseline_imgs)\n",
    "task_series = masker.fit_transform(task_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline_series.shape)\n",
    "print(task_series.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrix_baseline = correlation_measure.fit_transform([baseline_series])[0]\n",
    "\n",
    "correlation_matrix_task = correlation_measure.fit_transform([task_series])[0]\n",
    "\n",
    "# Plot the correlation matrix\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "# Make a large figure\n",
    "# Mask the main diagonal for visualization:\n",
    "np.fill_diagonal(correlation_matrix_baseline, 0)\n",
    "# The labels we have start with the background (0), hence we skip the\n",
    "# first label\n",
    "# matrices are ordered for block-like representation\n",
    "plotting.plot_matrix(correlation_matrix_baseline, figure=(10, 8), labels=labels[1:],\n",
    "                     vmax=0.8, vmin=-0.8, reorder=True)\n",
    "\n",
    "# Make a large figure\n",
    "# Mask the main diagonal for visualization:\n",
    "np.fill_diagonal(correlation_matrix_task, 0)\n",
    "# The labels we have start with the background (0), hence we skip the\n",
    "# first label\n",
    "# matrices are ordered for block-like representation\n",
    "plotting.plot_matrix(correlation_matrix_task, figure=(10, 8), labels=labels[1:],\n",
    "                     vmax=0.8, vmin=-0.8, reorder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_connectome(correlation_matrix_baseline, coordinates,\n",
    "                         title='baseline corr',\n",
    "                         edge_threshold=\"90%\",edge_cmap=\"copper\", colorbar=\"true\")\n",
    "\n",
    "plotting.plot_connectome(correlation_matrix_task, coordinates,\n",
    "                         title='task corr',\n",
    "                         edge_threshold=\"90%\",edge_cmap=\"copper\", colorbar=\"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
